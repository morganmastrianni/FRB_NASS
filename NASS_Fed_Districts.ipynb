{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NASS Statistics by Federal Reserve Bank District\n",
    "\n",
    "Which Federal Reserve Bank district has the most farmland? What commodities generate the most income in each district? Which district sells the most llamas? Using USDA 2022 Ag Census data and a nifty shapefile compiled by Colton Tousey of the Federal Reserve Bank of Kansas City, we can answer all of these questions, and more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import requests as req\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import json\n",
    "from alive_progress import alive_bar\n",
    "from time import sleep\n",
    "from wakepy import keep\n",
    "import polars.selectors as cs\n",
    "from great_tables import GT, style, loc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Gathering Data\n",
    "\n",
    "***DON'T RUN, it will take 3.5 hrs with good wifi***\n",
    "\n",
    "First, we need an API key from the USDA to query the NASS database. This is an untracked file in the GitHub repository for this project; it needs to be independently requested from the USDA by whoever wants to run this code.\n",
    "\n",
    "FedCounties.csv records the Federal Reserve Bank district for every county in the United States, along with state and county FIPS codes. NASS statistics also include FIPS codes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fed_counties_df = pl.read_csv(\"FedCounties.csv\")\n",
    "fed_counties_df = fed_counties_df.filter(\n",
    "    ~pl.col(\"STATEFP\").is_in([78,72,69,66,60])\n",
    ")\n",
    "tuples = []\n",
    "\n",
    "for dist in range(1, 13):\n",
    "    filtered = fed_counties_df.filter(fed_counties_df[\"District\"] == dist)\n",
    "    tuples.extend(\n",
    "        zip(\n",
    "            filtered[\"District\"].to_list(),\n",
    "            filtered[\"STATEFP\"].to_list(),\n",
    "            filtered[\"COUNTYFP\"].to_list(),\n",
    "        )\n",
    "    )\n",
    "\n",
    "tuples = [(t[0], str(t[1]).zfill(2), str(t[2]).zfill(3)) for t in tuples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3142\n"
     ]
    }
   ],
   "source": [
    "print(len(tuples))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have the unique county, state, Fed district pairs. The next step is to gather ALL 2022 Census data for every county and add a new variable to the USDA data: \"District\".\n",
    "\n",
    "***DON'T RUN, continue from 2***\n",
    "\n",
    "Note: Puerto Rico and the U.S. Virgin Islands are excluded (part of the N.Y. Fed district), there was trouble with querying those state FIPS codes..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "url = \"https://quickstats.nass.usda.gov/api/api_GET\"\n",
    "api_key = os.getenv(\"NASS_api_key\")\n",
    "\n",
    "district_dfs = []\n",
    "with keep.presenting():  # took approx. 3:29 hrs\n",
    "    for dist in range(1, 13):\n",
    "        pairs = [\n",
    "            (state, county) for district, state, county in tuples if district == dist\n",
    "        ]\n",
    "\n",
    "        county_dfs = []\n",
    "        with alive_bar(len(pairs), title=\"Pairs\") as bar:\n",
    "            for state, county in pairs:\n",
    "                bar()\n",
    "                raw = req.get(\n",
    "                    url,\n",
    "                    params={\n",
    "                        \"key\": api_key,\n",
    "                        \"state_fips_code\": state,\n",
    "                        \"county_code\": county,\n",
    "                        \"agg_level_desc\": \"COUNTY\",\n",
    "                        \"source_desc\": \"CENSUS\",\n",
    "                        \"year\": 2022,\n",
    "                        \"format\": \"json\",\n",
    "                    },\n",
    "                ).text\n",
    "                sleep(2)\n",
    "\n",
    "                try:\n",
    "                    content = json.loads(raw)\n",
    "                except json.decoder.JSONDecodeError as e:\n",
    "                    print(raw)\n",
    "                    raise e\n",
    "\n",
    "                if \"error\" in content:\n",
    "                    print(state, county)\n",
    "                    print(content[\"error\"])\n",
    "                    continue\n",
    "\n",
    "                county_df = pl.DataFrame(json.loads(raw)[\"data\"])\n",
    "                county_df = county_df.select(\n",
    "                    [pl.col(c) for c in sorted(county_df.columns)]\n",
    "                )\n",
    "                county_dfs.append(county_df)\n",
    "\n",
    "        district_df = pl.concat(county_dfs)\n",
    "        district_df = district_df.with_columns(pl.lit(dist).alias(\"District\"))\n",
    "        district_dfs.append(district_df)\n",
    "\n",
    "NASS_pull = pl.concat(district_dfs)\n",
    "NASS_pull.write_parquet(\"NASS_pull.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collect data on Puerto Rico in a separate query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (45_633, 40)\n",
      "┌────────┬─────────────┬────────────────┬──────────┬───┬─────────────┬──────┬───────┬──────────┐\n",
      "│ CV (%) ┆ Value       ┆ agg_level_desc ┆ asd_code ┆ … ┆ week_ending ┆ year ┆ zip_5 ┆ District │\n",
      "│ ---    ┆ ---         ┆ ---            ┆ ---      ┆   ┆ ---         ┆ ---  ┆ ---   ┆ ---      │\n",
      "│ str    ┆ str         ┆ str            ┆ str      ┆   ┆ str         ┆ i64  ┆ str   ┆ i32      │\n",
      "╞════════╪═════════════╪════════════════╪══════════╪═══╪═════════════╪══════╪═══════╪══════════╡\n",
      "│ 22.1   ┆ 28,813,951  ┆ PUERTO RICO &  ┆          ┆ … ┆             ┆ 2022 ┆       ┆ 2        │\n",
      "│        ┆             ┆ OUTLYING AREAS ┆          ┆   ┆             ┆      ┆       ┆          │\n",
      "│ 8.0    ┆ 48,301,595  ┆ PUERTO RICO &  ┆          ┆ … ┆             ┆ 2022 ┆       ┆ 2        │\n",
      "│        ┆             ┆ OUTLYING AREAS ┆          ┆   ┆             ┆      ┆       ┆          │\n",
      "│ 11.0   ┆ 41,205,033  ┆ PUERTO RICO &  ┆          ┆ … ┆             ┆ 2022 ┆       ┆ 2        │\n",
      "│        ┆             ┆ OUTLYING AREAS ┆          ┆   ┆             ┆      ┆       ┆          │\n",
      "│ 13.1   ┆ 71,583,387  ┆ PUERTO RICO &  ┆          ┆ … ┆             ┆ 2022 ┆       ┆ 2        │\n",
      "│        ┆             ┆ OUTLYING AREAS ┆          ┆   ┆             ┆      ┆       ┆          │\n",
      "│ 4.8    ┆ 107,130,970 ┆ PUERTO RICO &  ┆          ┆ … ┆             ┆ 2022 ┆       ┆ 2        │\n",
      "│        ┆             ┆ OUTLYING AREAS ┆          ┆   ┆             ┆      ┆       ┆          │\n",
      "│ …      ┆ …           ┆ …              ┆ …        ┆ … ┆ …           ┆ …    ┆ …     ┆ …        │\n",
      "│ 16.9   ┆ 49          ┆ PUERTO RICO &  ┆          ┆ … ┆             ┆ 2022 ┆       ┆ 2        │\n",
      "│        ┆             ┆ OUTLYING AREAS ┆          ┆   ┆             ┆      ┆       ┆          │\n",
      "│ 7.1    ┆ 173         ┆ PUERTO RICO &  ┆          ┆ … ┆             ┆ 2022 ┆       ┆ 2        │\n",
      "│        ┆             ┆ OUTLYING AREAS ┆          ┆   ┆             ┆      ┆       ┆          │\n",
      "│ 9.5    ┆ 439         ┆ PUERTO RICO &  ┆          ┆ … ┆             ┆ 2022 ┆       ┆ 2        │\n",
      "│        ┆             ┆ OUTLYING AREAS ┆          ┆   ┆             ┆      ┆       ┆          │\n",
      "│ 19.0   ┆ 15          ┆ PUERTO RICO &  ┆          ┆ … ┆             ┆ 2022 ┆       ┆ 2        │\n",
      "│        ┆             ┆ OUTLYING AREAS ┆          ┆   ┆             ┆      ┆       ┆          │\n",
      "│ 17.9   ┆ 14          ┆ PUERTO RICO &  ┆          ┆ … ┆             ┆ 2022 ┆       ┆ 2        │\n",
      "│        ┆             ┆ OUTLYING AREAS ┆          ┆   ┆             ┆      ┆       ┆          │\n",
      "└────────┴─────────────┴────────────────┴──────────┴───┴─────────────┴──────┴───────┴──────────┘\n"
     ]
    }
   ],
   "source": [
    "# load_dotenv()\n",
    "# url = \"https://quickstats.nass.usda.gov/api/api_GET\"\n",
    "# api_key = os.getenv(\"NASS_api_key\")\n",
    "\n",
    "raw = req.get(\n",
    "    url,\n",
    "    params={\n",
    "        \"key\": api_key,\n",
    "        \"agg_level_desc\": \"PUERTO RICO & OUTLYING AREAS\",\n",
    "        \"state_name\": \"PUERTO RICO\",\n",
    "        \"source_desc\": \"CENSUS\",\n",
    "        \"year\": 2022,\n",
    "        \"format\": \"json\",\n",
    "    },\n",
    ").text\n",
    "\n",
    "try:\n",
    "    content = json.loads(raw)\n",
    "except json.decoder.JSONDecodeError as e:\n",
    "    print(raw)\n",
    "    raise e\n",
    "\n",
    "pr_df = pl.DataFrame(json.loads(raw)[\"data\"])\n",
    "pr_df = pr_df.select([pl.col(c) for c in sorted(pr_df.columns)])\n",
    "NASS_pull_pr = pr_df.with_columns(pl.lit(2).alias(\"District\"))\n",
    "NASS_pull_pr.write_parquet(\"NASS_pull_pr.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final dataset is stored as a .parquet file; this is very similar to a CSV file but it takes up a fraction of the space. There are over 3 million rows in \"NASS_pull.parquet; a CSV file with that many rows costs actual money to upload to GitHub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (499_133, 40)\n",
      "┌────────┬──────────┬────────────────┬──────────┬───┬─────────────┬──────┬───────┬──────────┐\n",
      "│ CV (%) ┆ Value    ┆ agg_level_desc ┆ asd_code ┆ … ┆ week_ending ┆ year ┆ zip_5 ┆ District │\n",
      "│ ---    ┆ ---      ┆ ---            ┆ ---      ┆   ┆ ---         ┆ ---  ┆ ---   ┆ ---      │\n",
      "│ str    ┆ str      ┆ str            ┆ str      ┆   ┆ str         ┆ i64  ┆ str   ┆ i32      │\n",
      "╞════════╪══════════╪════════════════╪══════════╪═══╪═════════════╪══════╪═══════╪══════════╡\n",
      "│ 10.0   ┆ 42,907   ┆ COUNTY         ┆ 10       ┆ … ┆             ┆ 2022 ┆       ┆ 1        │\n",
      "│ 7.2    ┆ 675      ┆ COUNTY         ┆ 10       ┆ … ┆             ┆ 2022 ┆       ┆ 1        │\n",
      "│ (L)    ┆ 100      ┆ COUNTY         ┆ 10       ┆ … ┆             ┆ 2022 ┆       ┆ 1        │\n",
      "│ 1.5    ┆ 94       ┆ COUNTY         ┆ 10       ┆ … ┆             ┆ 2022 ┆       ┆ 1        │\n",
      "│ 8.2    ┆ 1,362    ┆ COUNTY         ┆ 10       ┆ … ┆             ┆ 2022 ┆       ┆ 1        │\n",
      "│ …      ┆ …        ┆ …              ┆ …        ┆ … ┆ …           ┆ …    ┆ …     ┆ …        │\n",
      "│ 10.1   ┆ 51       ┆ COUNTY         ┆ 10       ┆ … ┆             ┆ 2022 ┆       ┆ 12       │\n",
      "│ 21.7   ┆ 29,739   ┆ COUNTY         ┆ 10       ┆ … ┆             ┆ 2022 ┆       ┆ 12       │\n",
      "│ 11.6   ┆ 86       ┆ COUNTY         ┆ 10       ┆ … ┆             ┆ 2022 ┆       ┆ 12       │\n",
      "│ 17.2   ┆ -230,000 ┆ COUNTY         ┆ 10       ┆ … ┆             ┆ 2022 ┆       ┆ 12       │\n",
      "│ 20.3   ┆ -1,682   ┆ COUNTY         ┆ 10       ┆ … ┆             ┆ 2022 ┆       ┆ 12       │\n",
      "└────────┴──────────┴────────────────┴──────────┴───┴─────────────┴──────┴───────┴──────────┘\n"
     ]
    }
   ],
   "source": [
    "df_big = pl.read_parquet(\"NASS_pull.parquet\")\n",
    "df_pr = pl.read_parquet(\"NASS_pull_pr.parquet\")\n",
    "dfs = [df_big, df_pr]\n",
    "df = pl.concat(dfs)\n",
    "df_equine = df.filter(pl.col(\"short_desc\").str.contains(\"PRODUCERS\"))\n",
    "print(df_equine)\n",
    "df_equine.write_parquet(\"equine.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Cleaning\n",
    "\n",
    "Some values in the final dataset are not actual values, so we need to filter these rows out. Then, we can aggregate our data to get rid of extraneous information, which at this point is any and all columns excluding \"short_desc\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (20_611, 3)\n",
      "┌─────────────────────────────────┬─────────────────────────────────┬──────────┐\n",
      "│ short_desc                      ┆ District_Total                  ┆ District │\n",
      "│ ---                             ┆ ---                             ┆ ---      │\n",
      "│ str                             ┆ list[f64]                       ┆ i32      │\n",
      "╞═════════════════════════════════╪═════════════════════════════════╪══════════╡\n",
      "│ TREE NUT TOTALS - OPERATIONS W… ┆ [109.0, 109.0, … 109.0]         ┆ 1        │\n",
      "│ PIGEONS & SQUAB - SALES, MEASU… ┆ [328.0, 328.0]                  ┆ 1        │\n",
      "│ LETTUCE, LEAF, FRESH MARKET - … ┆ [452.0, 452.0, … 452.0]         ┆ 1        │\n",
      "│ ESCAROLE & ENDIVE, FRESH MARKE… ┆ [135.0, 135.0, … 135.0]         ┆ 1        │\n",
      "│ TURKEYS - OPERATIONS WITH INVE… ┆ [802.0, 802.0, … 802.0]         ┆ 1        │\n",
      "│ …                               ┆ …                               ┆ …        │\n",
      "│ PRODUCERS, LAND USE OR CROP DE… ┆ [276989.0, 276989.0, … 276989.… ┆ 12       │\n",
      "│ PRODUCERS, AMERICAN INDIAN OR … ┆ [8260.0, 8260.0, … 8260.0]      ┆ 12       │\n",
      "│ AG LAND, PASTURELAND, (EXCL CR… ┆ [83180.0, 83180.0, … 83180.0]   ┆ 12       │\n",
      "│ LEGUMES, ALFALFA, SEED - PRODU… ┆ [3.3110119e7, 3.3110119e7, … 3… ┆ 12       │\n",
      "│ CHESTNUTS - OPERATIONS WITH AR… ┆ [209.0, 209.0, … 209.0]         ┆ 12       │\n",
      "└─────────────────────────────────┴─────────────────────────────────┴──────────┘\n"
     ]
    }
   ],
   "source": [
    "df_big = pl.read_parquet(\"NASS_pull.parquet\")\n",
    "df_pr = pl.read_parquet(\"NASS_pull_pr.parquet\")\n",
    "dfs = [df_big, df_pr]\n",
    "df = pl.concat(dfs)\n",
    "df = df.filter(\n",
    "    (~pl.col(\"Value\").str.contains(r\"\\(D\\)|\\(Z\\)\")) & (pl.col(\"domain_desc\") == \"TOTAL\")\n",
    ")\n",
    "df = df.with_columns(pl.col(\"Value\").str.replace_all(\",\", \"\").cast(pl.Float64))\n",
    "\n",
    "# want to know if there are multiple of the same short_desc entries per county...\n",
    "# want to know if filtering by domain_desc == \"TOTAL\" ensures that there are ONLY unique short_descs for each county...\n",
    "# grouped_df = df.group_by(['state_fips_code', 'county_code', 'short_desc']).agg(\n",
    "#     pl.len().alias('count')\n",
    "# )\n",
    "\n",
    "# # Now find which state_fips_code and county_code combinations have duplicated short_desc values\n",
    "# # We group again by state_fips_code and county_code, and filter where any short_desc appears more than once\n",
    "# result = grouped_df.group_by(['state_fips_code', 'county_code']).agg([\n",
    "#     pl.len().alias('unique_short_desc_count'),\n",
    "#     (pl.col('count') > 1).any().alias('has_duplicate_short_desc')\n",
    "# ]).filter(\n",
    "#     pl.col('has_duplicate_short_desc') == True\n",
    "# )\n",
    "\n",
    "# print(result)\n",
    "# print(result[\"unique_short_desc_count\"].sum())\n",
    "    \n",
    "# thank you Claude\n",
    "\n",
    "district_dfs = []\n",
    "\n",
    "for dist in df.partition_by(\"District\"):\n",
    "    district_df = dist.group_by(\"short_desc\").agg(\n",
    "        [\n",
    "            pl.when(pl.col(\"short_desc\").str.contains(\"PCT\"))\n",
    "            .then(pl.col(\"Value\").median())\n",
    "            .otherwise(pl.col(\"Value\").sum())\n",
    "            .alias(\"District_Total\"),\n",
    "            pl.mean(\"District\").cast(pl.Int32),\n",
    "        ]\n",
    "    )\n",
    "    district_dfs.append(district_df)\n",
    "\n",
    "df = pl.concat(district_dfs)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: We take the median percentages (robust to outliers) across all counties in the dataset. The interpretation of these values is not super intuitive. Each mean percent is the \"average percent ___ for all counties in the district\", not the percent ___ for the district. We also make sure that domain_desc = \"TOTAL\" or else we double-count some values.\n",
    "\n",
    "Also, the conditional aggregation creates a dataframe where \"District_Total\" is actually a column of lists. We resolve this in step 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Analyzing\n",
    "\n",
    "***RUN FROM HERE***\n",
    "\n",
    "To filter through the data and find commodities that we want to know more about, we can use a keyword search approach applied to the short description of the data item. Some examples are presented below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "districts = range(1, 13)\n",
    "# districts = [10, 11, 12]\n",
    "\n",
    "keyword_list = []\n",
    "excl_keyword_list = []\n",
    "\n",
    "# k1 = [\"acres\", \"ag land\"]\n",
    "# keyword_list.append(k1)\n",
    "# ek1 = [\n",
    "#     \"treated\",\n",
    "#     \"wood\",\n",
    "#     \"pasture\",\n",
    "#     \"reserv\",\n",
    "#     \"to\",\n",
    "#     \"crop\",\n",
    "#     \"pct\",\n",
    "#     \"irrigated\",\n",
    "#     \"organic\",\n",
    "# ]\n",
    "# excl_keyword_list.append(ek1)\n",
    "\n",
    "# k2 = [\"number\", \"ag land\"]\n",
    "# keyword_list.append(k2)\n",
    "# ek2 = [\"wood\", \"pasture\", \"reserv\", \"to\", \"crop\", \"pct\", \"irrigated\", \"organic\"]\n",
    "# excl_keyword_list.append(ek2)\n",
    "\n",
    "# k3 = [\"number\", \"asset value\", \"\\$\"]\n",
    "# keyword_list.append(k3)\n",
    "# ek3 = []\n",
    "# excl_keyword_list.append(ek3)\n",
    "\n",
    "# k4 = [\"income\", \"receipts\", \"\\$\"]\n",
    "# keyword_list.append(k4)\n",
    "# ek4 = [\"operation\", \"other\", \"dividends\", \"insurance\", \"forest\", \"tourism\"]\n",
    "# excl_keyword_list.append(ek4)\n",
    "\n",
    "# k5 = [\"income\", \"net\", \"\\$\"]\n",
    "# keyword_list.append(k5)\n",
    "# ek5 = [\"gain\", \"loss\", \"/ operation\"]\n",
    "# excl_keyword_list.append(ek5)\n",
    "\n",
    "commodity_keywords = [\"sales, measured in \\$\"]\n",
    "keyword_list.append(commodity_keywords)\n",
    "commodity_excl_keywords = [\"totals\"]\n",
    "excl_keyword_list.append(commodity_excl_keywords)\n",
    "\n",
    "dfs = []\n",
    "\n",
    "for incl, excl in zip(keyword_list, excl_keyword_list):\n",
    "    custom = df.filter(\n",
    "        [pl.col(\"short_desc\").str.to_lowercase().str.contains(k.lower()) for k in incl],\n",
    "        *[\n",
    "            ~pl.col(\"short_desc\").str.to_lowercase().str.contains(exk.lower())\n",
    "            for exk in excl\n",
    "        ],\n",
    "        pl.col(\"District\").is_in(districts),\n",
    "    )\n",
    "    dfs.append(custom)\n",
    "\n",
    "custom = pl.concat(dfs)\n",
    "custom = custom.with_columns(pl.col(\"District_Total\").list.unique().list.first())\n",
    "\n",
    "custom.write_parquet(\"custom_df.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we know exactly which data items we would like included in a final table, then we can move on to 4. If some extra analysis needs doing, then go to step 3a first."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3a. More Analyzing\n",
    "\n",
    "If there are some secondary characteristics we want more information on, such as which commodities generate the most cash sales in each district, then some more work needs to be done before a dataframe will be ready for final formatting. Below we find the top 10 highest value commodities in each district, per our earlier keyword search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (180, 3)\n",
      "┌─────────────────────────────────┬────────────────┬──────────┐\n",
      "│ short_desc                      ┆ District_Total ┆ District │\n",
      "│ ---                             ┆ ---            ┆ ---      │\n",
      "│ str                             ┆ f64            ┆ i32      │\n",
      "╞═════════════════════════════════╪════════════════╪══════════╡\n",
      "│ MILK - SALES, MEASURED IN $     ┆ 9.46258e8      ┆ 1        │\n",
      "│ FIELD CROPS, OTHER, INCL HAY -… ┆ 3.16334e8      ┆ 1        │\n",
      "│ CATTLE, INCL CALVES - SALES, M… ┆ 1.38262e8      ┆ 1        │\n",
      "│ MAPLE SYRUP - SALES, MEASURED … ┆ 1.37478e8      ┆ 1        │\n",
      "│ GRAIN - SALES, MEASURED IN $    ┆ 6.4153e7       ┆ 1        │\n",
      "│ …                               ┆ …              ┆ …        │\n",
      "│ BARLEY - SALES, MEASURED IN $   ┆ 3.58206e8      ┆ 12       │\n",
      "│ PROPAGATIVE MATERIAL - SALES, … ┆ 3.3460312e8    ┆ 12       │\n",
      "│ CUT FLOWERS & CUT CULTIVATED G… ┆ 3.23013307e8   ┆ 12       │\n",
      "│ HOGS - SALES, MEASURED IN $     ┆ 2.52975e8      ┆ 12       │\n",
      "│ CUT CHRISTMAS TREES & SHORT TE… ┆ 2.12468e8      ┆ 12       │\n",
      "└─────────────────────────────────┴────────────────┴──────────┘\n"
     ]
    }
   ],
   "source": [
    "df = pl.read_parquet(\"custom_df.parquet\")\n",
    "\n",
    "district_dfs = []\n",
    "\n",
    "for dist in df.partition_by(\"District\"):\n",
    "    district_df = dist.sort(\"District_Total\", descending=True).head(15)\n",
    "    district_dfs.append(district_df)\n",
    "\n",
    "df = pl.concat(district_dfs)\n",
    "print(df)\n",
    "\n",
    "df.write_parquet(\"custom_df.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Table Formatting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict = {\n",
    "    \"short_desc\": \"Description\",\n",
    "    \"1\": \"Boston\",\n",
    "    \"2\": \"New York (excl. PR and U.S. VI)\",\n",
    "    \"3\": \"Philadelphia\",\n",
    "    \"4\": \"Cleveland\",\n",
    "    \"5\": \"Richmond\",\n",
    "    \"6\": \"Atlanta\",\n",
    "    \"7\": \"Chicago\",\n",
    "    \"8\": \"St. Louis\",\n",
    "    \"9\": \"Minneapolis\",\n",
    "    \"10\": \"Kansas City\",\n",
    "    \"11\": \"Dallas\",\n",
    "    \"12\": \"San Francisco\",\n",
    "}\n",
    "\n",
    "df = pl.read_parquet(\"custom_df.parquet\")\n",
    "\n",
    "df = df.pivot(\"District\", values=cs.starts_with(\"District_Total\"))\n",
    "\n",
    "df = df.rename(dict)\n",
    "df = df.with_columns(pl.col(\"Description\").str.to_titlecase())\n",
    "df.write_csv(\"custom_table.csv\")\n",
    "# print(df)\n",
    "\n",
    "# # refer to NASS for units\n",
    "# gt_df = GT(df)\n",
    "\n",
    "# dist_cols = [\n",
    "#     \"Boston\",\n",
    "#     \"New York\",\n",
    "#     \"Philadelphia\",\n",
    "#     \"Cleveland\",\n",
    "#     \"Richmond\",\n",
    "#     \"Atlanta\",\n",
    "#     \"Chicago\",\n",
    "#     \"St. Louis\",\n",
    "#     \"Minneapolis\",\n",
    "#     \"Kansas City\",\n",
    "#     \"Dallas\",\n",
    "#     \"San Francisco\",\n",
    "# ]\n",
    "\n",
    "# gt_df = gt_df.tab_spanner(label=\"District\", columns=dist_cols).tab_style(\n",
    "#     style=style.text(size=\"9px\", font=\"Helvetica\"),\n",
    "#     locations=loc.body(columns=\"Description\"),\n",
    "# )\n",
    "\n",
    "# gt_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From here, I think a good amount of hard-coding is needed for table formatting; districts will have different top-production commodities, so how do we want to display that information? It's tougher to decide than when you are comparing particular commodity classes across districts..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
